[PATHS]
athena_path=/home/zvladimi/ATHENA/src/
pickled_path=/home/zvladimi/ATHENA/pickle_data/
ml_dset_path=/home/zvladimi/ATHENA/ML_dsets/
path_to_models=/home/zvladimi/ATHENA/xgboost_results/
debug_plt_path=/home/zvladimi/ATHENA/Random_figs/
rockstar_ctlgs_path=/home/zvladimi/ATHENA/

[SNAP_DATA]
snap_path=/home/zvladimi/ATHENA/SPARTA/sparta_data/snaps/
# If known can put the snapshots, this is NEEDED if snapshot data is not provided
# If unknown just leave as an empty list
# Must be put in order of primary snapshot then secondary snapshot
known_snaps=[190,164]
# how are the snapshots formatted with regards to their number of 0s 
snap_dir_format={:04d}
snap_format={:04d}

[SPARTA_DATA]
sparta_output_path=/home/zvladimi/ATHENA/SPARTA/sparta_output/
curr_sparta_file=cbol_l0063_n0256_4r200m_1-5v200m

[MISC]
random_seed=11
# Chunksize for python multiprocessing
mp_chunk_size=250
# Options anything default in colossus or planck13-nbody
sim_cosmol=bolshoi
# If turned on will print general debug information
debug_gen=0
# If turned on will print memory information for gen_ML_datasets.py
debug_mem=0
# plot indivdual halo density profiles produced by the ML model compared to SPARTA when using gen_ML_dsets or test_xgboost. 
# The number entered is the number of halos compared (starting from the largest down)
debug_indiv_dens_prf=0

# If turned on halo and particle data and particle trees will be saved in pickle files to allow for faster results on multiple runs.
# This is advised to be turned on if you are running the code on the same dataset multiple times, 
# if not or you do not have enough storage space turn it off, but there will be an additional overhead each run
#TODO rename since now not just pickles
save_intermediate_data=1

# RESET LEVELS for gen_ML_datasets.py (no impact on train_xgboost.py)
# 0: no reset will just run from the beginning or continue from where the last run left off
# 1: Removes the calculated information (ptl_info and halo_info) and redoes all the calculations
# Beyond 1 REQUIRES snapshot data to be present ASSUMING pickle_data was turned on 
# 2: Same as 1 and removes the the particle trees and the number of particles per halo
# 3: Same as 2 and removes all pickled data about the halos and particles from SPARTA/the simulation
reset_search=3

# RETRAIN LEVELS for train_xgboost.py
# 0: no retraining will just predict with current model (if there is one otherwise will train a new one) on test dataset
# 1: will retrain model but use old parameters (if they exist)
# 2: will retrain model with new parameters
retrain_model=2

[DSET_CREATE]
# save size for each pd dataframe that is saved to HDF5 File
# The corresponding HDF5 will likely be a bit bigger depending on the size of the PD df
sub_dset_mem_size=1e8
# The redshift of the primary snapshot. Will find the closest redshift to what is inputted so no need for an exact number
input_z=0.03
# Only calculated from the redshift in input_z, the number of dynamical times that the snapshot should be separated from that first redshift
tdyn_steps=[0.5]
#TODO be able to adjust the radius metric
# In R200m search for particles around halo centers. Ideally will match what SPARTA's calculated profiles go out to
search_radius=4
# The fraction of the dataset that is for the testing data  
test_dset_frac=0.25
val_dset_frac=0.10

[DASK_CLIENT]
# Used for setting up dask cluster. If running on an HPC (with SLURM) this will be taken from the environment otherwie it can be set here
dask_task_ncpus=4
on_zaratan=0
use_gpu=1

[TRAIN_MODEL]

features=["Scaled_radii","Radial_vel","Tangential_vel"]
target_column=["Orbit_infall"]

# The maximum number of particle files to be loaded per simulation. Used to roughly balance the amount of data used in the model
# from each simulation. gen_ML_datasets should have used the same hdf5_mem_size param for each sim. To not use this simply enter 0
file_lim=1

# Should be full name of the simulation calculated info folder
# THE ORDER OF SIMULATIONS MATTERS FOR REFERENCING HALOS
# determines what model is trained in train_xgboost.py and what model performs the preds in test_xgboost.py
model_sims=["cbol_l0063_n0256_4r200m_1-5v200m_182_148"]

# Name is important as that is how separate models for the same dataset can be referenced
model_type=base_flim1

# Creates plots showing the accuracy of the model depending on the number of trees used as well as how fast predictions are made with the number of trees.
# This is done before the model is saved (thus it is done in train_model.py) as the history is still available
tree_err=0

[EVAL_MODEL]

# only used in test_xgboost.py. Should be a list of lists, each sublist is the sims used for a singular model.
# [[l0063_n1024,l1000_n1024]] is one model using 2 sims while [[l0063_n1024],[l1000_n1024]] is two models each using 1 sim
test_sims=[["cbol_l0063_n0256_4r200m_1-5v200m_182_148"]] 

# Options are "Full", "Train", "Test". Can be any combination of the three
# This is used in reference to model_sims' dataset in train_xgboost.py and test_sims' dataset in test_xgboost.py
eval_datasets=["Test"]

# What plots to make
dens_prf_plt     = 1
fulldist_plt     = 1
misclass_plt     = 1

# Determine for density profile plots where splits in nu (peak height) or macc (mass accretion rate) should be made and what should be plotted
dens_prf_nu_split=1
dens_prf_macc_split=1
plt_nu_splits=0.5-1,1-1.5,1.5-2,2-3,3-6
# Can also split by mass accretion rate
plt_macc_splits=0.5-1,1-1.5,1.5-2,2-3,3-6,6-10
# the minimum number of halos that must be present in the the nu bin to be plotted
min_halo_nu_bin=25

# For the missclass and fulldist plots where both linear and log scales are used can set
# the threshold for linear (from -thrsh to thrsh) and then the number of linear bins and log bins
linthrsh=3
lin_nbin=30
# THIS SHOULD BE DIVISIBLE BY 2 IF THERE ARE NEG and POS LOG BINS
log_nbin=20 

# List the ticks that will be displayed. The location for imshow plots is automatically calculated.  
# rv ticks are mirrored for negative values
# Leave blank if not using
lin_rvticks=[0,1,2,3]
log_rvticks=[6,12]
lin_tvticks=[0,0.5,1,2,3]
log_tvticks=[5,7.5,12]
lin_rticks=[0,0.5,1,2,3,4]
log_rticks=[]

#TODO add reset calibration parameter
[KE_CUT]

fast_ke_calib_sims=["cbol_l0063_n0256_4r200m_1-5v200m_182_166"]
opt_ke_calib_sims=["cbol_l0063_n0256_4r200m_1-5v200m_182_166"]
ke_test_sims=[["cbol_l0063_n0256_4r200m_1-5v200m_182_166"]]

# Parameters used to control the calibration of the fast phase space cut model
n_points=20
perc=0.99
width=0.05
grad_lims=(0.2,0.5)
# The radius in R200m beyond which particles are not considered for calibration
r_cut_calib=1.75

# The radius in R200m beyond which all particles are considered infalling
r_cut_pred=2.0

