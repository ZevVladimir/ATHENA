[PATHS]
MLOIS_path: /home/zvladimi/MLOIS/src/
pickled_path: /home/zvladimi/MLOIS/pickle_data/
ML_dset_path: /home/zvladimi/MLOIS/ML_dsets/
path_to_models: /home/zvladimi/MLOIS/xgboost_results/
debug_plt_path: /home/zvladimi/MLOIS/Random_figs/

[SNAP_DATA]
#TODO make this more generalized
snap_path: /home/zvladimi/MLOIS/SPARTA/sparta_data/snaps/
# If known can put the snapshots, this is NEEDED if snapshot data is not provided
# If unknown just leave as an empty list
# Must be put in order of primary snapshot then secondary snapshot
known_snaps:[184,160]
# how are the snapshots formatted with regards to their number of 0s 
snap_dir_format={:04d}
snap_format={:04d}

[SPARTA_DATA]
#TODO make this more generalized
SPARTA_output_path: /home/zvladimi/MLOIS/SPARTA/sparta_output/
curr_sparta_file:cbol_l0063_n0256_4r200m_1-5v200m

[MISC]
random_seed=11
# Chunksize for python multiprocessing
mp_chunk_size=250
# Options anything default in colossus or planck13-nbody
sim_cosmol=bolshoi
# If turned on will print general debug information
debug_gen=0
# If turned on will print memory information for gen_ML_datasets.py
debug_mem=0

# If turned on halo and particle data and particle trees will be saved in pickle files to allow for faster results on multiple runs.
# This is advised to be turned on if you are running the code on the same dataset multiple times, 
# if not or you do not have enough storage space turn it off, but there will be an additional overhead each run
pickle_data=1

# RESET LEVELS for gen_ML_datasets.py (no impact on train_xgboost.py)
# 0: no reset will just run from the beginning or continue from where the last run left off
# 1: Removes the calculated information (ptl_info and halo_info) and redoes all the calculations
# Beyond 1 REQUIRES snapshot data to be present
# 2: Same as 1 and removes the the particle trees and the number of particles per halo
# 3: Same as 2 and removes all pickled data about the halos and particles from SPARTA/the simulation
reset_search = 3

# RETRAIN LEVELS for train_xgboost.py
# 0: no retraining will just predict with current model (if there is one otherwise will train a new one) on test dataset
# 1: will retrain model but use old parameters (if they exist)
# 2: will retrain model with new parameters
retrain_model = 2

[DSET_CREATE]
# save size for each pd dataframe that is saved to HDF5 File
# The corresponding HDF5 will likely be a bit bigger depending on the size of the PD df
sub_dset_mem_size = 2.5e9
# The redshift of the primary snapshot. Will find the closest redshift to what is inputted so no need for an exact number
p_red_shift=-0.07
# How many dynamical times should the snapshots be separated
t_dyn_step=1
#TODO be able to adjust the radius metric
# In R200m search for particles around halo centers. Ideally will match what SPARTA's calculated profiles go out to
search_radius=4
# The fraction of the dataset that is for the testing data  
test_dset_frac=0.25

[DASK_CLIENT]
# Used for setting up dask cluster. If running on an HPC (with SLURM) this will be taken from the environment otherwie it can be set here
dask_task_ncpus=4
on_zaratan=0
use_gpu=1

[TRAIN_MODEL]

feature_columns = ["p_Scaled_radii","p_Radial_vel","p_Tangential_vel","c_Scaled_radii","c_Radial_vel","c_Tangential_vel"]
target_column = ["Orbit_infall"]

# The maximum number of particle files to be loaded per simulation. Used to roughly balance the amount of data used in the model
# from each simulation. gen_ML_datasets should have used the same hdf5_mem_size param for each sim. To not use this simply enter 0
file_lim = 1

# Should be full name of the simulation calculated info folder
# THE ORDER OF SIMULATIONS MATTERS FOR REFERENCING HALOS
# determines what model is trained in train_xgboost.py and what model performs the preds in test_xgboost.py
model_sims:["cbol_l0063_n0256_4r200m_1-5v200m_190to164"]

# Name is important as that is how separate models for the same dataset can be referenced
model_type:base_flim1

[EVAL_MODEL]

# only used in test_xgboost.py. Should be a list of lists, each sublist is the sims used for a singular model.
# [[l0063_n1024,l1000_n1024]] is one model using 2 sims while [[l0063_n1024],[l1000_n1024]] is two models each using 1 sim
test_sims:[["cbol_l0063_n0256_4r200m_1-5v200m_190to164"]] 

#TODO Check "Full" dataset actually works
# Options are "Full", "Train", "Test". Can be any combination of the three
# This is used in reference to model_sims' dataset in train_xgboost.py and test_sims' dataset in test_xgboost.py
eval_datasets:["Test"]

# What plots to make
dens_prf_plt     = 1 
fulldist_plt     = 1
misclass_plt     = 1
io_frac_plt      = 0

# Determine for density profile plots where splits in nu should be made and what should be plotted
dens_prf_nu_split = 1
plt_nu_splits=0.5-1,1-1.5,1.5-2,2-3,3-6
# Can also split by mass accretion rate
plt_macc_splits=0.5-1,1-1.5,1.5-2,2-3,3-6,6-10

# For the missclass and fulldist plots where both linear and log scales are used can set
# the threshold for linear (from -thrsh to thrsh) and then the number of linear bins and log bins
linthrsh = 3
lin_nbin = 30
# THIS SHOULD BE DIVISIBLE BY 2 IF THERE ARE NEG and POS LOG BINS
log_nbin = 20 

# List the ticks that will be displayed. The location for imshow plots is automatically calculated.  
# rv ticks are mirrored for negative values
# Leave blank if not using
lin_rvticks = [0,0.5,1,2,3]
log_rvticks = [5,7.5,12]
lin_tvticks = [0,0.5,1,2,3]
log_tvticks = [5,7.5,12]
lin_rticks = [0,0.5,1,2,3,4]
log_rticks = []

#TODO add parameters for phase space cut code
[PS_Cut]


#####################################################
[OPTIMIZE]
# THE FOLLOWING PARAMTERS CORRESPOND TO THINGS NOT FULLY IMPLEMENTED AND/OR TESTED

# TRAIN DATASET ADJUSTMENT PARAMS
# for each adjustment set any (or all) of the params to 0 for these adjustments to not be performed

# The following two parameters adjust the number of particles used in the training. This is done by dividing the radii into log bins, setting a maximum particle
# number based off the total number of particles within a radii and then limiting every following bin to that maximum number of particles.

# reduce_rad takes any radius (>0) and will set the amount of particles within that radii as the maximum number of particles per following radius bin
# reduce_perc takes a decimal and will scale the maximum amount 
reduce_rad = 0
reduce_perc = 0.001

# You can also determine a radius after which orbiting particles will start to be weighted less on an exponential curve (less important the further out)
# weighting is of form: weights = e^((ln(min_weight)/(max_rad-weight_rad)) * (rad - weight_rad))
# weight_rad determines the radius at which this weighting starts (all particles with smaller radii have weights of 1)
# min_weight determines the lowest weight at the furthest radius. 
weight_rad = 0
min_weight = 0.01
weight_exp = 10

# Perform hyperparameter tuning on the weighting of the dataset. Overwrites the weight_rad/min_weight parameter
opt_weights = 0
opt_scale_rad = 0

# Mark as 1 even if the hpo model has already been trained
hpo=0
# Options are:
# all: accuracy on all particles
# orb: accuracy on only orbiting particles
# inf: accuracy on only infalling particles (not recommended all does basically the same)
# mprf_all: accuracy on infalling + orbiting mass profiles
# mprf_orb: accuracy on only orbiting mass profile
hpo_loss = "all"

