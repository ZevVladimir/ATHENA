[PATHS]
MLOIS_path: /home/zvladimi/MLOIS/src/
pickled_path: /home/zvladimi/MLOIS/pickle_data/
ML_dset_path: /home/zvladimi/MLOIS/ML_dsets/
path_to_models: /home/zvladimi/MLOIS/xgboost_results/
snap_path: /home/zvladimi/MLOIS/SPARTA/sparta_data/snaps/
SPARTA_output_path: /home/zvladimi/MLOIS/SPARTA/sparta_output/
path_to_pygadgetreader: /home/zvladimi/MLOIS/.pygadgetreader/

[MISC]
curr_sparta_file:cbol_l0063_n0256_4r200m_1-5v200m
# how are the snapshots formatted with regards to their number of 0s 
snap_dir_format={:04d}
snap_format={:04d}
random_seed=11
on_zaratan=0
use_gpu=1
# Options anything default in colossus or planck13-nbody
sim_cosmol=bolshoi
# If turned on will print general debug information
debug_gen=0
# If turned on will print memory information for gen_ML_datasets.py
debug_mem=0

# If turned on halo and particle data and particle trees will be saved in pickle files to allow for faster results on multiple runs.
# This is advised to be turned on if you are running the code on the same dataset multiple times, 
# if not or you do not have enough storage space turn it off, but there will be an additional overhead each run
pickle_data=1

[SEARCH]
# RESET LEVELS for gen_ML_datasets.py (no impact on train_xgboost.py)
# 0: no reset will just run from the beginning or continue from where the last run left off
# 1: Removes the calculated information (ptl_info and halo_info) and redoes all the calculations
# 2: Same as 1 and removes the the particle trees and the number of particles per halo
# 3: Same as 2 and removes all pickled data about the halos and particles from SPARTA/the simulation
reset = 3

# The redshift of the primary snapshot. Will find the closest redshift to what is inputted so no need for an exact number
p_red_shift=-0.07
# Only want one snapshot? If not will find an additional snapshot in the past as specified by t_dyn_step below
prim_snap_only=0
# How many dynamical times should the snapshots be separated
t_dyn_step=1
# In R200m search for particles around halo centers. Ideally will match what SPARTA's profiles go out to
search_radius=4
total_num_snaps=193
# save size for each pd dataframe that is saved to HDF5 File
# The corresponding HDF5 will likely be a bit bigger depending on the size of the PD df
save_mem_size = 2.5e9
# Chunksize for multiprocessing
chunk_size=250

# How many things are being saved about the particles
# If for gen_ML_datasets 7: halo_first, halo_n, HPIDS, Orbit/Infall, Radius, Rad Vel, Tang Vel, 
# If for morb_cat 6: Halo_ID, Halo_pos, Halo_vel, M_orb, M200m, R200m
# IMPORTANT: if adjusting this you should adjust calc_halo_mem in calculation_functions.py such that the right amount of memory is calculated
num_save_ptl_params=7

[XGBOOST]
# RETRAIN LEVELS for train_xgboost.py
# 0: no retraining will just predict with current model (if there is one otherwise will train a new one) on test dataset
# 1: will retrain model but use old parameters (if they exist)
# 2: will retrain model with new parameters
retrain = 2
feature_columns = ["p_Scaled_radii","p_Radial_vel","p_Tangential_vel","c_Scaled_radii","c_Radial_vel","c_Tangential_vel"]
target_column = ["Orbit_infall"]

# Used for setting up dask cluster. If running on an HPC (with SLURM) this will be taken from the environment otherwie it can be set here
dask_task_cpus=4

# The maximum number of particle files to be loaded per simulation. Used to roughly balance the amount of data used in the model
# from each simulation. gen_ML_datasets should have used the same hdf5_mem_size param for each sim. To not use this simply enter 0
file_lim = 1

# Should be full name of the simulation calculated info folder
# THE ORDER OF SIMULATIONS MATTERS FOR REFERENCING HALOS
# determines what model is trained in train_xgboost.py and what model performs the preds in test_xgboost.py
model_sims:["cbol_l0063_n0256_4r200m_1-5v200m_190to164"]
#model_sims:["cbol_l0063_n1024_4r200m_1-5v200m_100to90","cbol_l0125_n1024_4r200m_1-5v200m_100to90","cbol_l0250_n1024_4r200m_1-5v200m_100to90","cbol_l0500_n1024_4r200m_1-5v200m_100to90","cbol_l1000_n1024_4r200m_1-5v200m_100to90","cbol_l2000_n1024_4r200m_1-5v200m_100to90"] 
test_halos_ratio=0.25

# only used in test_xgboost.py. Should be a list of lists, each sublist is the sims used for a singular model.
# [[l0063_n1024,l1000_n1024]] is one model using 2 sims while [[l0063_n1024],[l1000_n1024]] is two models each using 1 sim
test_sims:[["cbol_l0063_n0256_4r200m_1-5v200m_190to164"]] 
# Name is important as that is how separate models for the same dataset can be referenced
model_type:base_flim1
#TODO Check "Full" dataset actually works
# Options are "Full", "Train", "Test". Can be any combination of the three
# This is used in reference to model_sims' dataset in train_xgboost.py and test_sims' dataset in test_xgboost.py
eval_datasets:["Test"]

# What plots to make
dens_prf_plt     = 1 
fulldist_plt     = 1
misclass_plt     = 1
io_frac_plt      = 0

# Determine for density profile plots where splits in nu should be made and what should be plotted
dens_prf_nu_split = 1
plt_nu_splits=0.5-1,1-1.5,1.5-2,2-3,3-6

# For the missclass and fulldist plots where both linear and log scales are used can set
# the threshold for linear (from -thrsh to thrsh) and then the number of linear bins and log bins

linthrsh = 3
lin_nbin = 30
# THIS SHOULD BE DIVISIBLE BY 2 IF THERE ARE NEG and POS LOG BINS
log_nbin = 20 

# List the ticks that will be displayed. The location for imshow plots is automatically calculated.  
# rv ticks are mirrored for negative values
# Leave blank if not using
lin_rvticks = [0,0.5,1,2,3]
log_rvticks = [5,7.5,12]
lin_tvticks = [0,0.5,1,2,3]
log_tvticks = [5,7.5,12]
lin_rticks = [0,0.5,1,2,3,4]
log_rticks = []


#####################################################
# THE FOLLOWING PARAMTERS CORRESPOND TO THINGS NOT FULLY IMPLEMENTED AND/OR TESTED

# TRAIN DATASET ADJUSTMENT PARAMS
# for each adjustment set any (or all) of the params to 0 for these adjustments to not be performed

# The following two parameters adjust the number of particles used in the training. This is done by dividing the radii into log bins, setting a maximum particle
# number based off the total number of particles within a radii and then limiting every following bin to that maximum number of particles.

# reduce_rad takes any radius (>0) and will set the amount of particles within that radii as the maximum number of particles per following radius bin
# reduce_perc takes a decimal and will scale the maximum amount 
reduce_rad = 0
reduce_perc = 0.001

# You can also determine a radius after which orbiting particles will start to be weighted less on an exponential curve (less important the further out)
# weighting is of form: weights = e^((ln(min_weight)/(max_rad-weight_rad)) * (rad - weight_rad))
# weight_rad determines the radius at which this weighting starts (all particles with smaller radii have weights of 1)
# min_weight determines the lowest weight at the furthest radius. 
weight_rad = 0
min_weight = 0.01
weight_exp = 10

# Perform hyperparameter tuning on the weighting of the dataset. Overwrites the weight_rad/min_weight parameter
opt_wghts = 0
opt_scale_rad = 0

# Mark as 1 even if the hpo model has already been trained
hpo=0
# Options are:
# all: accuracy on all particles
# orb: accuracy on only orbiting particles
# inf: accuracy on only infalling particles (not recommended all does basically the same)
# mprf_all: accuracy on infalling + orbiting mass profiles
# mprf_orb: accuracy on only orbiting mass profile
hpo_loss = "all"
# the radius that the training dataset will be created up to
# the testing dataset will use all data
training_rad=5
rad_splits=0 # not currently implemented
frac_train_data=1

# Nu splits is untested but would define which ranges of nus you want to be included in your dataset.
nu_splits=0-10